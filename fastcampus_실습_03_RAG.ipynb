{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9477fac",
   "metadata": {
    "id": "b9477fac"
   },
   "source": [
    "\n",
    "# RAG 실습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd64888",
   "metadata": {
    "id": "fdd64888"
   },
   "source": [
    "\n",
    "## 0) 설치 & 환경 설정\n",
    "\n",
    "- Retrieval: sentence-transformers, faiss-cpu, rank-bm25\n",
    "- Generation: transformers\n",
    "- Framework: langchain-*\n",
    "- Compression: llmlingua\n",
    "- Evaluation: ragas, datasets\n",
    "-  openai key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe91c3b",
   "metadata": {
    "executionInfo": {
     "elapsed": 11774,
     "status": "ok",
     "timestamp": 1763004514091,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "9fe91c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip -q install sentence-transformers faiss-cpu rank-bm25 transformers langchain-text-splitters\n",
    "!pip -q install \"langchain==0.2.14\" \"langchain-community==0.2.12\" \"langchain-core>=0.2.0\"\n",
    "!pip -q install llmlingua\n",
    "!pip -q install ragas datasets\n",
    "!pip -q install openai\n",
    "\n",
    "import random, numpy as np\n",
    "random.seed(42); np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51988ed",
   "metadata": {
    "id": "b51988ed"
   },
   "source": [
    "\n",
    "## 1) 금융 코퍼스(토이) & 라벨\n",
    "\n",
    "간단 문단 텍스트 + 라벨(topic, tags) 구성\n",
    "나중에 자신의 도메인 문서(사내 위키/FAQ/리포트)로 교체 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed7a178",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1763007679297,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "1ed7a178",
    "outputId": "26b7d094-31f7-46bc-e4cb-ecad61dfbae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#문단 수: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "finance_docs = [\n",
    "    (\"삼성전자 메모리 부문은 DDR5 전환과 HBM 수요 확대에 힘입어 수익성이 회복되고 있다.\", {\"topic\":\"samsung\", \"tags\":[\"memory\",\"hbm\",\"profitability\"]}),\n",
    "    (\"SK하이닉스는 HBM3E 양산을 통해 엔비디아와의 파트너십을 강화하고 매출 다변화를 꾀한다.\", {\"topic\":\"skhynix\", \"tags\":[\"hbm\",\"nvidia\",\"revenue\"]}),\n",
    "    (\"엔비디아는 데이터센터용 GPU 출하가 확대되어 분기 최대 실적을 갱신했다.\", {\"topic\":\"nvidia\", \"tags\":[\"gpu\",\"datacenter\",\"earnings\"]}),\n",
    "    (\"TSMC는 3나노 공정 수율 개선으로 하이엔드 고객 주문을 확보하며 파운드리 점유율을 높였다.\", {\"topic\":\"tsmc\", \"tags\":[\"foundry\",\"3nm\",\"yield\"]}),\n",
    "    (\"애플은 A시리즈 칩 자체 설계를 강화하고 TSMC와의 공조로 성능과 전력 효율을 개선한다.\", {\"topic\":\"apple\", \"tags\":[\"iphone\",\"asoc\",\"tsmc\"]}),\n",
    "    (\"미국의 기준금리 동결 기조가 지속될 경우, 성장주 밸류에이션은 점진적 회복세를 보일 수 있다.\", {\"topic\":\"macro\", \"tags\":[\"rates\",\"valuation\",\"growth\"]}),\n",
    "    (\"원/달러 환율 상승은 반도체 수출 채산성에 단기적으로 긍정적 영향을 준다.\", {\"topic\":\"fx\", \"tags\":[\"krw\",\"usd\",\"semiconductor\",\"exports\"]}),\n",
    "    (\"HBM 채택 확대는 LLM 추론 효율과 메모리 대역폭 병목을 동시에 개선한다.\", {\"topic\":\"hbm\", \"tags\":[\"bandwidth\",\"llm\",\"inference\"]}),\n",
    "    (\"반도체 공급망 다변화는 지정학적 리스크를 관리하는 핵심 전략으로 자리 잡고 있다.\", {\"topic\":\"supplychain\", \"tags\":[\"geopolitics\",\"risk\",\"diversification\"]}),\n",
    "    (\"서버 DRAM 수요는 AI 워크로드 증가와 함께 중장기 성장 추세가 지속될 전망이다.\", {\"topic\":\"drams\", \"tags\":[\"server\",\"ai\",\"demand\"]}),\n",
    "]\n",
    "docs_texts = [d[0] for d in finance_docs]\n",
    "docs_labels = [d[1] for d in finance_docs]\n",
    "print(f\"#문단 수: {len(docs_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4c9e8",
   "metadata": {
    "id": "98e4c9e8"
   },
   "source": [
    "\n",
    "## 2) 질의 세트 & 정답 기준(needs)\n",
    "\n",
    "각 질의에 대해 needs = {topic, tags} 를 설정하여 관련성 판정에 사용\n",
    "실제 과제에서는 별도의 정답 문장(ground truth)을 마련하는 것을 권장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f266806",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1763007683989,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "1f266806",
    "outputId": "e61a53aa-4c07-4213-a196-b74749767690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#질의 수: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "queries = [\n",
    "    {\"q\": \"HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘\", \"needs\": {\"tags\":[\"hbm\",\"llm\",\"inference\"]}},\n",
    "    {\"q\": \"엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘\", \"needs\": {\"topic\":[\"nvidia\"], \"tags\":[\"gpu\",\"datacenter\",\"earnings\"]}},\n",
    "    {\"q\": \"삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘\", \"needs\": {\"topic\":[\"samsung\",\"skhynix\"], \"tags\":[\"hbm\"]}},\n",
    "    {\"q\": \"미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘\", \"needs\": {\"topic\":[\"macro\"], \"tags\":[\"rates\",\"valuation\",\"growth\"]}},\n",
    "    {\"q\": \"환율 변화가 반도체 수출에 미치는 영향\", \"needs\": {\"topic\":[\"fx\"], \"tags\":[\"krw\",\"usd\",\"semiconductor\",\"exports\"]}},\n",
    "]\n",
    "print(f\"#질의 수: {len(queries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43b482",
   "metadata": {
    "id": "db43b482"
   },
   "source": [
    "\n",
    "## 3) 청킹(Chunking): Manual vs LangChain\n",
    "\n",
    "- Manual: NLTK 문장 분리 -> 슬라이딩 윈도우(Overlap)\n",
    "- LangChain: RecursiveCharacterTextSplitter 로 길이 기반 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bUQOI0V8QsVb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1763004569729,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "bUQOI0V8QsVb",
    "outputId": "af653a46-df4d-448e-8349-9f5f9b3c791c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41e8111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1763007688253,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "f41e8111",
    "outputId": "ddd9d437-dc86-4600-f25a-b40d840c72ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual chunks 예시:\n",
      " 삼성전자 메모리 부문은 DDR5 전환과 HBM 수요 확대에 힘입어 수익성이 회복되고 있다.\n",
      "---\n",
      "SK하이닉스는 HBM3E 양산을 통해 엔비디아와의 파트너십을 강화하고 매출 다변화를 꾀한다.\n",
      "---\n",
      "엔비디아는 데이터센터용 GPU 출하가 확대되어 분기 최대 실적을 갱신했다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk, itertools\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sent_chunk(text):\n",
    "    \"한 문단을 문장 단위로 분리\"\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def sliding_window(sents, w=2, overlap=1):\n",
    "    \"\"\"\n",
    "    문장 리스트(sents)를 길이 w로 겹치게 분할\n",
    "    overlap=1이면 이전 청크와 1문장 겹침\n",
    "    \"\"\"\n",
    "    chunks=[]; step=max(1, w-overlap)\n",
    "    for i in range(0, max(1, len(sents)-w+1), step):\n",
    "        chunks.append(\" \".join(sents[i:i+w]))\n",
    "    return chunks\n",
    "\n",
    "chunks_manual = list(itertools.chain.from_iterable(sliding_window(sent_chunk(d), w=2, overlap=1) for d in docs_texts))\n",
    "print(\"Manual chunks 예시:\\n\", \"\\n---\\n\".join(chunks_manual[:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2457f63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1763007689712,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "a2457f63",
    "outputId": "89f00cd1-35f0-46c6-ab8c-7ea4323daaa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain chunks 예시:\n",
      " 삼성전자 메모리 부문은 DDR5 전환과 HBM 수요 확대에 힘입어 수익성이 회복되고 있다.\n",
      "\n",
      "SK하이닉스는 HBM3E 양산을 통해 엔비디아와의 파트너십을 강화하고 매출 다변화를 꾀한다.\n",
      "\n",
      "엔비디아는 데이터센터용 GPU 출하가 확대되어 분기 최대 실적을 갱신했다.\n",
      "\n",
      "TSMC는 3나노 공정 수율 개선으로 하이엔드 고객 주문을 확보하며 파운드리 점유율을 높였다.\n",
      "---\n",
      "애플은 A시리즈 칩 자체 설계를 강화하고 TSMC와의 공조로 성능과 전력 효율을 개선한다.\n",
      "\n",
      "미국의 기준금리 동결 기조가 지속될 경우, 성장주 밸류에이션은 점진적 회복세를 보일 수 있다.\n",
      "\n",
      "원/달러 환율 상승은 반도체 수출 채산성에 단기적으로 긍정적 영향을 준다.\n",
      "\n",
      "HBM 채택 확대는 LLM 추론 효율과 메모리 대역폭 병목을 동시에 개선한다.\n",
      "---\n",
      "반도체 공급망 다변화는 지정학적 리스크를 관리하는 핵심 전략으로 자리 잡고 있다.\n",
      "\n",
      "서버 DRAM 수요는 AI 워크로드 증가와 함께 중장기 성장 추세가 지속될 전망이다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=220, chunk_overlap=40, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks_lc = text_splitter.split_text(\"\\n\\n\".join(docs_texts))\n",
    "print(\"LangChain chunks 예시:\\n\", \"\\n---\\n\".join(chunks_lc[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd99b32",
   "metadata": {
    "id": "6bd99b32"
   },
   "source": [
    "\n",
    "## 4) 쿼리 리라이팅/분해(규칙 기반)\n",
    "\n",
    "복합 질의를 의도별 서브쿼리로 분해 -> 각 서브쿼리 검색 결과를 max-pool 로 합성\n",
    "규칙 예시: 기업 비교(삼성전자 vs 하이닉스), HBM/LLM/GPU/금리/환율 키워드 확장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b463c685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "error",
     "timestamp": 1763007693837,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "b463c685",
    "outputId": "efadc3b1-1270-4813-ce17-2a806109939c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 -> ['HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘', 'HBM LLM 추론 성능 영향', 'HBM vs DDR 메모리 성능 비교', 'LLM 추론 성능 최적화 방법', 'HBM 메모리 대역폭', 'GPU 성능과 HBM의 관계', 'LLM 모델 크기와 HBM의 영향']\n",
      "엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 -> ['엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘', '엔비디아 GPU 수요', '엔비디아 데이터센터 실적', 'GPU 수요 증가 원인', '데이터센터 실적 향상 요인', '엔비디아 GPU vs AMD GPU', '엔비디아 데이터센터 실적 vs 경쟁사 실적']\n",
      "삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 -> ['삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘', '삼성전자 HBM 전략', '하이닉스 HBM 전략', '삼성전자와 하이닉스 HBM 전략 비교', 'HBM 기술 차이 삼성전자 하이닉스', 'HBM 시장 점유율 삼성전자 하이닉스']\n",
      "미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 -> ['미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘', '미국 금리와 성장주 밸류에이션 관계', '미국 금리 변화가 성장주에 미치는 영향', '성장주 밸류에이션과 금리의 상관관계', '금리 인상과 성장주 가치 평가', '미국 금리 vs 성장주 밸류에이션', '금리와 주식 시장의 상관관계', '성장주와 금리 변화의 상호작용']\n",
      "환율 변화가 반도체 수출에 미치는 영향 -> ['환율 변화가 반도체 수출에 미치는 영향', '환율 변화 반도체 수출 영향', '환율 변화 HBM 반도체 수출', '환율 변화 LLM 반도체 수출', '환율 변화 GPU 반도체 수출', '환율 변화 금리 반도체 수출', '환율 상승 반도체 수출 감소 vs 환율 하락 반도체 수출 증가']\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API를 사용해 질의를 서브쿼리로 분해하는 예시\n",
    "# 사전 준비: `pip install openai` (최신 SDK), 환경변수 OPENAI_API_KEY 설정\n",
    "\n",
    "import os, json, time\n",
    "from typing import List\n",
    "from openai import OpenAI, APIError, RateLimitError\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 정보검색 어시스턴트입니다. 사용자의 한국어 질의를 \"\n",
    "    \"검색 친화적인 여러 '서브쿼리'로 분해합니다. \"\n",
    "    \"반드시 JSON 배열(list of strings)만 출력하세요. \"\n",
    "    \"예: [\\\"HBM LLM 추론 성능 영향\\\", \\\"메모리 대역폭 병목\\\"]\"\n",
    ")\n",
    "\n",
    "def decompose_with_openai(q: str, model: str = \"gpt-4o-mini\") -> List[str]:\n",
    "    \"\"\"\n",
    "    OpenAI 모델로 질의를 검색용 서브쿼리 리스트로 분해.\n",
    "    실패 시 원문 질의만 담긴 리스트를 반환.\n",
    "    \"\"\"\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": (\n",
    "                            \"질의:\\n\"\n",
    "                            f\"{q}\\n\\n\"\n",
    "                            \"지침:\\n\"\n",
    "                            \"- 비교/대조 의도가 보이면 각 엔티티별 서브쿼리와 'A vs B' 서브쿼리를 함께 만드세요.\\n\"\n",
    "                            \"- 금융 도메인 키워드(HBM, LLM, GPU, 금리, 환율 등)는 구체화하세요.\\n\"\n",
    "                            \"- 반드시 JSON 배열만 출력하세요.\"\n",
    "                        ),\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            text = resp.choices[0].message.content.strip()\n",
    "            # 모델이 코드블록으로 감싸거나 주석을 붙이는 경우를 대비해 JSON만 추출 시도\n",
    "            start = text.find(\"[\")\n",
    "            end = text.rfind(\"]\")\n",
    "            if start != -1 and end != -1:\n",
    "                text = text[start : end + 1]\n",
    "            subs = json.loads(text)\n",
    "            # 품질 필터: 문자열만 남기고 공백/중복 제거\n",
    "            subs = [s.strip() for s in subs if isinstance(s, str) and s.strip()]\n",
    "            # 최소 한 개는 보장\n",
    "            return subs if subs else [q]\n",
    "        except (json.JSONDecodeError, APIError, RateLimitError) as e:\n",
    "            # 간단한 백오프 후 재시도\n",
    "            time.sleep(0.8 * (attempt + 1))\n",
    "    return [q]\n",
    "\n",
    "\n",
    "def rule_rewrite(q: str) -> List[str]:\n",
    "    subs = decompose_with_openai(q)\n",
    "    # 원문 포함 + 중복 제거 (집합 보존 순서)\n",
    "    seen, out = set(), []\n",
    "    for s in [q] + subs:\n",
    "        if s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "# 데모: queries 리스트를 순회하며 분해 결과 출력\n",
    "for ex in queries:\n",
    "    print(ex[\"q\"], \"->\", rule_rewrite(ex[\"q\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d864cd5",
   "metadata": {
    "id": "6d864cd5"
   },
   "source": [
    "\n",
    "## 5) 임베딩 모델 로드 (옵션 NMIXX -> MiniLM)\n",
    "\n",
    "아래 NMIXX_HUB_ID 에 실제 레포 ID를 넣으면 NMIXX와 비교 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303b85da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13533,
     "status": "ok",
     "timestamp": 1763007716536,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "303b85da",
    "outputId": "3b6a73f7-8b57-4a0a-f1d6-372f0e4e63f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] nmixx-fin/nmixx-bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nmixx-fin/nmixx-bge-m3. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> ok in 5.5s\n",
      ">>> 사용 임베딩: nmixx-fin/nmixx-bge-m3\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, faiss, time\n",
    "\n",
    "NMIXX_HUB_ID = \"nmixx-fin/nmixx-bge-m3\"\n",
    "\n",
    "def load_any(try_list):\n",
    "    \"여러 후보를 순서대로 로드 시도 -> 성공 시 즉시 반환\"\n",
    "    last_err = None\n",
    "    for name in try_list:\n",
    "        try:\n",
    "            print(\"[LOAD]\", name)\n",
    "            start = time.time()\n",
    "            m = SentenceTransformer(name)\n",
    "            print(\" -> ok in %.1fs\" % (time.time()-start))\n",
    "            return name, m\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(\" -> failed:\", e)\n",
    "    raise last_err\n",
    "\n",
    "model_candidates = [NMIXX_HUB_ID, \"sentence-transformers/all-MiniLM-L6-v2\"]\n",
    "model_name, embed_model = load_any(model_candidates)\n",
    "print(\">>> 사용 임베딩:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fb772",
   "metadata": {
    "id": "7d4fb772"
   },
   "source": [
    "\n",
    "## 6) 검색 구현: Manual vs LangChain\n",
    "\n",
    "- Manual\n",
    "  - Vector: FAISS(IndexFlatIP) + 코사인 유사도(정규화 내적)\n",
    "  - BM25: rank-bm25\n",
    "  - Hybrid: 정규화 후 가중합 (lam*BM25 + (1-lam)*cosine)\n",
    "- LangChain\n",
    "  - FAISS VectorStore, BM25Retriever, EnsembleRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20eb07e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1763007716764,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "20eb07e9",
    "outputId": "6612b0a8-fed1-4d6f-9549-02c5067f5d40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_faiss(emb_model, texts):\n",
    "    \"텍스트 -> 임베딩 -> FAISS IndexFlatIP 빌드\"\n",
    "    vec = emb_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    index = faiss.IndexFlatIP(vec.shape[1]); index.add(vec)\n",
    "    return index, vec\n",
    "\n",
    "index_manual, vec_manual = build_faiss(embed_model, docs_texts)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenized = [nltk.word_tokenize(t) for t in docs_texts]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "def cosine_scores(emb_model, index, query):\n",
    "    \"쿼리 하나에 대해 전체 문서 코사인 유사도 점수 배열 반환\"\n",
    "    qv = emb_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D,I = index.search(qv, len(docs_texts))\n",
    "    scores = np.zeros(len(docs_texts)); scores[I[0]] = D[0]\n",
    "    return scores\n",
    "\n",
    "def bm25_scores(query):\n",
    "    \"BM25 점수 배열 반환\"\n",
    "    return bm25.get_scores(nltk.word_tokenize(query))\n",
    "\n",
    "def hybrid_scores(emb_model, index, query, lam=0.4):\n",
    "    \"정규화된 가중합으로 BM25/코사인 결합\"\n",
    "    cos = cosine_scores(emb_model, index, query)\n",
    "    bm  = bm25_scores(query)\n",
    "    def norm(x): x = x.astype(np.float64); return (x - x.min())/(x.max()-x.min()+1e-9)\n",
    "    return lam*norm(bm) + (1-lam)*norm(cos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b115afb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2399,
     "status": "ok",
     "timestamp": 1763004993135,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "b115afb0",
    "outputId": "54d65485-b268-4bfc-d66f-d404d015e4ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nmixx-fin/nmixx-bge-m3. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangChain Hybrid top-3]\n",
      "query: {'q': 'HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘', 'needs': {'tags': ['hbm', 'llm', 'inference']}}\n",
      "- HBM 채택 확대는 LLM 추론 효율과 메모리 대역폭 병목을 동시에 개선한다.\n"
     ]
    }
   ],
   "source": [
    "# LangChain 구현 (개념 매핑)\n",
    "from langchain_community.vectorstores import FAISS as LC_FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever  \n",
    "\n",
    "emb_lc = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "vs = LC_FAISS.from_texts(docs_texts, embedding=emb_lc)\n",
    "retriever_vec = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "bm25_lc = BM25Retriever.from_texts(docs_texts)\n",
    "bm25_lc.k = 5\n",
    "\n",
    "hybrid_ret = EnsembleRetriever(\n",
    "    retrievers=[bm25_lc, retriever_vec],\n",
    "    weights=[0.4, 0.6],\n",
    ")\n",
    "\n",
    "print(\"[LangChain Hybrid top-3]\")\n",
    "print(f\"query: {queries[0]}\")\n",
    "for d in hybrid_ret.get_relevant_documents(queries[0][\"q\"])[:1]:\n",
    "    print(\"-\", d.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d6a78",
   "metadata": {
    "id": "209d6a78"
   },
   "source": [
    "\n",
    "## 7) 평가 지표 계산기 (P@k / R@k / MRR / nDCG@k)\n",
    "\n",
    "토이 라벨(topic/tags) 기반 이진 판정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe601247",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1763005003893,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "fe601247"
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_relevant(doc_meta, needs):\n",
    "    \"문서 메타와 needs(topic/tags)를 바탕으로 관련성 판정\"\n",
    "    ok = False\n",
    "    if \"topic\" in needs and doc_meta[\"topic\"] in set(needs[\"topic\"]):\n",
    "        ok = True\n",
    "    if \"tags\" in needs and set(needs[\"tags\"]).intersection(set(doc_meta[\"tags\"])):\n",
    "        ok = True\n",
    "    return ok\n",
    "\n",
    "def topk_by_scores(scores, k=5):\n",
    "    \"점수 배열에서 상위 k개 인덱스/점수 반환\"\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return order, scores[order]\n",
    "\n",
    "def metrics_at_k(ranked_idx, needs, k=5):\n",
    "    \"P@k, R@k, MRR, nDCG@k 계산\"\n",
    "    rel = [1 if is_relevant(docs_labels[i], needs) else 0 for i in ranked_idx[:k]]\n",
    "    p_at_k = sum(rel)/k\n",
    "    total_rel = sum(1 for i in range(len(docs_texts)) if is_relevant(docs_labels[i], needs))\n",
    "    r_at_k = sum(rel)/max(1, total_rel)\n",
    "    # MRR\n",
    "    mrr = 0.0\n",
    "    for rank, r in enumerate(rel, start=1):\n",
    "        if r==1: mrr = 1.0/rank; break\n",
    "    # nDCG@k\n",
    "    import math\n",
    "    dcg = sum((rel[i]/math.log2(i+2)) for i in range(len(rel)))\n",
    "    ideal = sorted(rel, reverse=True)\n",
    "    idcg = sum((ideal[i]/math.log2(i+2)) for i in range(len(ideal)))\n",
    "    ndcg = dcg/(idcg+1e-9)\n",
    "    return {\"P@%d\"%k: p_at_k, \"R@%d\"%k: r_at_k, \"MRR\": mrr, \"nDCG@%d\"%k: ndcg}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae2268",
   "metadata": {
    "id": "f3ae2268"
   },
   "source": [
    "\n",
    "## 8) 통합 평가 루프 (Rewrite + Vector/BM25/Hybrid)\n",
    "\n",
    "서브쿼리별 점수를 max-pool 로 합성 -> 각 모드(Vector/BM25/Hybrid) 지표 계산\n",
    "lam=0.3/0.5/0.7 변화에 따른 Hybrid 성능 변화를 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ead93ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 12124,
     "status": "ok",
     "timestamp": 1763005108812,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "2ead93ed",
    "outputId": "fd70aba2-30ab-43b1-9e3d-c1d952a479ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>rewrite_subqs</th>\n",
       "      <th>mode</th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘</td>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘</td>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...</td>\n",
       "      <td>hybrid_0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘</td>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘</td>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...</td>\n",
       "      <td>hybrid_0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘</td>\n",
       "      <td>HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘</td>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘</td>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...</td>\n",
       "      <td>hybrid_0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘</td>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘</td>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...</td>\n",
       "      <td>hybrid_0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘</td>\n",
       "      <td>미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘</td>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘</td>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...</td>\n",
       "      <td>hybrid_0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.919721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘</td>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.919721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘</td>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...</td>\n",
       "      <td>hybrid_0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.919721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘</td>\n",
       "      <td>삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘</td>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘</td>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...</td>\n",
       "      <td>hybrid_0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘</td>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘</td>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...</td>\n",
       "      <td>hybrid_0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘</td>\n",
       "      <td>엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향</td>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...</td>\n",
       "      <td>bm25</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향</td>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...</td>\n",
       "      <td>hybrid_0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향</td>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향</td>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...</td>\n",
       "      <td>hybrid_0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향</td>\n",
       "      <td>환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query  \\\n",
       "1    HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘   \n",
       "2    HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘   \n",
       "3    HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘   \n",
       "4    HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘   \n",
       "0    HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘   \n",
       "16   미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘   \n",
       "17   미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘   \n",
       "18   미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘   \n",
       "19   미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘   \n",
       "15   미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘   \n",
       "11         삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘   \n",
       "12         삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘   \n",
       "13         삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘   \n",
       "14         삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘   \n",
       "10         삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘   \n",
       "6   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘   \n",
       "7   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘   \n",
       "8   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘   \n",
       "9   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘   \n",
       "5   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘   \n",
       "21               환율 변화가 반도체 수출에 미치는 영향   \n",
       "22               환율 변화가 반도체 수출에 미치는 영향   \n",
       "23               환율 변화가 반도체 수출에 미치는 영향   \n",
       "24               환율 변화가 반도체 수출에 미치는 영향   \n",
       "20               환율 변화가 반도체 수출에 미치는 영향   \n",
       "\n",
       "                                        rewrite_subqs        mode  P@5  \\\n",
       "1   HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...        bm25  0.4   \n",
       "2   HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...  hybrid_0.3  0.6   \n",
       "3   HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...  hybrid_0.5  0.6   \n",
       "4   HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...  hybrid_0.7  0.6   \n",
       "0   HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘 | HBM LLM 추론...      vector  0.6   \n",
       "16  미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...        bm25  0.2   \n",
       "17  미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...  hybrid_0.3  0.2   \n",
       "18  미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...  hybrid_0.5  0.2   \n",
       "19  미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...  hybrid_0.7  0.2   \n",
       "15  미국 금리와 성장주 밸류에이션의 관계를 보여주는 문맥 찾아줘 | 미국 금리와 성장주...      vector  0.2   \n",
       "11  삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...        bm25  0.2   \n",
       "12  삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...  hybrid_0.3  0.4   \n",
       "13  삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...  hybrid_0.5  0.4   \n",
       "14  삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...  hybrid_0.7  0.4   \n",
       "10  삼성전자와 하이닉스의 HBM 전략 차이를 비교해줘 | 삼성전자 HBM 전략 | 하이...      vector  0.4   \n",
       "6   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...        bm25  0.2   \n",
       "7   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...  hybrid_0.3  0.2   \n",
       "8   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...  hybrid_0.5  0.2   \n",
       "9   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...  hybrid_0.7  0.2   \n",
       "5   엔비디아의 GPU 수요와 데이터센터 실적 관련 배경을 설명해줘 | 엔비디아 GPU ...      vector  0.2   \n",
       "21  환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...        bm25  0.2   \n",
       "22  환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...  hybrid_0.3  0.2   \n",
       "23  환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...  hybrid_0.5  0.2   \n",
       "24  환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...  hybrid_0.7  0.2   \n",
       "20  환율 변화가 반도체 수출에 미치는 영향 | 환율 변화 반도체 수출 영향 | 환율 변...      vector  0.2   \n",
       "\n",
       "         R@5  MRR    nDCG@5  \n",
       "1   0.666667  1.0  1.000000  \n",
       "2   1.000000  1.0  0.946902  \n",
       "3   1.000000  1.0  0.946902  \n",
       "4   1.000000  1.0  0.946902  \n",
       "0   1.000000  1.0  1.000000  \n",
       "16  1.000000  1.0  1.000000  \n",
       "17  1.000000  1.0  1.000000  \n",
       "18  1.000000  1.0  1.000000  \n",
       "19  1.000000  1.0  1.000000  \n",
       "15  1.000000  1.0  1.000000  \n",
       "11  0.500000  1.0  1.000000  \n",
       "12  1.000000  1.0  0.919721  \n",
       "13  1.000000  1.0  0.919721  \n",
       "14  1.000000  1.0  0.919721  \n",
       "10  1.000000  1.0  1.000000  \n",
       "6   1.000000  1.0  1.000000  \n",
       "7   1.000000  1.0  1.000000  \n",
       "8   1.000000  1.0  1.000000  \n",
       "9   1.000000  1.0  1.000000  \n",
       "5   1.000000  1.0  1.000000  \n",
       "21  1.000000  1.0  1.000000  \n",
       "22  1.000000  1.0  1.000000  \n",
       "23  1.000000  1.0  1.000000  \n",
       "24  1.000000  1.0  1.000000  \n",
       "20  1.000000  1.0  1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def eval_query(qobj, lam_list=(0.3,0.5,0.7), k=5, use_rewrite=True):\n",
    "    q = qobj[\"q\"]; needs = qobj[\"needs\"]\n",
    "    subs = rule_rewrite(q) if use_rewrite else [q]\n",
    "    vec = np.zeros(len(docs_texts)); bm = np.zeros(len(docs_texts))\n",
    "    for sub in subs:\n",
    "        vec = np.maximum(vec, cosine_scores(embed_model, index_manual, sub))\n",
    "        bm  = np.maximum(bm, bm25_scores(sub))\n",
    "    res = {}\n",
    "    order,_ = topk_by_scores(vec, k=k); res[\"vector\"] = metrics_at_k(order, needs, k=k)\n",
    "    order,_ = topk_by_scores(bm, k=k);  res[\"bm25\"]   = metrics_at_k(order, needs, k=k)\n",
    "    for lam in lam_list:\n",
    "        hyb = (\n",
    "            lam * ((bm - bm.min()) / (np.ptp(bm) + 1e-9)) +\n",
    "            (1 - lam) * ((vec - vec.min()) / (np.ptp(vec) + 1e-9))\n",
    "        )\n",
    "        order,_ = topk_by_scores(hyb, k=k); res[f\"hybrid_{lam}\"] = metrics_at_k(order, needs, k=k)\n",
    "    return res, subs\n",
    "\n",
    "rows = []\n",
    "for qobj in queries:\n",
    "    res, subs = eval_query(qobj, lam_list=(0.3,0.5,0.7), k=5, use_rewrite=True)\n",
    "    for mode, m in res.items():\n",
    "        row = {\"query\": qobj[\"q\"], \"rewrite_subqs\": \" | \".join(subs), \"mode\": mode}\n",
    "        row.update(m)\n",
    "        rows.append(row)\n",
    "df_eval = pd.DataFrame(rows).sort_values([\"query\",\"mode\"])\n",
    "df_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c15353",
   "metadata": {
    "id": "71c15353"
   },
   "source": [
    "\n",
    "## 9) 리랭커 (Cross-Encoder)\n",
    "\n",
    "Hybrid 상위 후보를 교차인코더로 재정렬 -> 정밀도 향상 기대\n",
    "수업에서는 경량 모델 권장: BAAI/bge-reranker-v2-m3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fa88d3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2069,
     "status": "ok",
     "timestamp": 1763007785767,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "2fa88d3a",
    "outputId": "2b7463e8-4783-4a26-9ae3-c268b936e59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 'HBM이 LLM 추론 성능에 주는 영향과 관련된 근거 찾아줘', 'needs': {'tags': ['hbm', 'llm', 'inference']}}\n",
      "[Before]\n",
      " HBM 채택 확대는 LLM 추론 효율과 메모리 대역폭 병목을 동시에 개선한다.\n",
      "---\n",
      "삼성전자 메모리 부문은 DDR5 전환과 HBM 수요 확대에 힘입어 수익성이 회복되고 있다.\n",
      "---\n",
      "원/달러 환율 상승은 반도체 수출 채산성에 단기적으로 긍정적 영향을 준다.\n",
      "---\n",
      "SK하이닉스는 HBM3E 양산을 통해 엔비디아와의 파트너십을 강화하고 매출 다변화를 꾀한다.\n",
      "---\n",
      "반도체 공급망 다변화는 지정학적 리스크를 관리하는 핵심 전략으로 자리 잡고 있다.\n",
      "\n",
      "[After Rerank]\n",
      " HBM 채택 확대는 LLM 추론 효율과 메모리 대역폭 병목을 동시에 개선한다.\n",
      "---\n",
      "삼성전자 메모리 부문은 DDR5 전환과 HBM 수요 확대에 힘입어 수익성이 회복되고 있다.\n",
      "---\n",
      "SK하이닉스는 HBM3E 양산을 통해 엔비디아와의 파트너십을 강화하고 매출 다변화를 꾀한다.\n",
      "---\n",
      "반도체 공급망 다변화는 지정학적 리스크를 관리하는 핵심 전략으로 자리 잡고 있다.\n",
      "---\n",
      "원/달러 환율 상승은 반도체 수출 채산성에 단기적으로 긍정적 영향을 준다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "demo_q = queries[0]\n",
    "hyb = hybrid_scores(embed_model, index_manual, demo_q[\"q\"], lam=0.5)\n",
    "cand_idx = np.argsort(hyb)[::-1][:5]\n",
    "cand = [docs_texts[i] for i in cand_idx]\n",
    "\n",
    "rerank_model = \"BAAI/bge-reranker-v2-m3\" # reranker\n",
    "\n",
    "rer = CrossEncoder(rerank_model)\n",
    "pairs = [[demo_q[\"q\"], c] for c in cand]\n",
    "scores = rer.predict(pairs)\n",
    "order2 = scores.argsort()[::-1]\n",
    "cand_rr = [cand[i] for i in order2]\n",
    "print(demo_q)\n",
    "print(\"[Before]\\n\", \"\\n---\\n\".join(cand))\n",
    "print(\"\\n[After Rerank]\\n\", \"\\n---\\n\".join(cand_rr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd2c45",
   "metadata": {
    "id": "d5bd2c45"
   },
   "source": [
    "\n",
    "## 10) 생성(압축 전)\n",
    "\n",
    "리랭크 상위 3개 문맥을 그대로 LLM에 주고 간단 답변 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24e7eda7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6342,
     "status": "ok",
     "timestamp": 1763007795554,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "24e7eda7",
    "outputId": "be35663f-b094-49d2-f827-f4d126afd4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[압축 전 답변]\n",
      " HBM 채택 확대는 LLM 추론 효율을 개선하여 성능을 향상시킵니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API를 이용해 생성 파트를 수행하는 버전\n",
    "# 사전 준비: pip install openai, OPENAI_API_KEY 환경변수 설정 필요\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    " # Google Colab Secrets 사용 시\n",
    "# from google.colab import userdata\n",
    "# api_key = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI()# (api_key=api_key)\n",
    "\n",
    "context_raw = \"\\n\\n\".join(cand_rr[:3])\n",
    "\n",
    "prompt = (\n",
    "    \"다음은 사용자의 질문에 답하기 위한 문맥입니다.\\n\"\n",
    "    \"문맥을 읽고 질문에 대한 간단하고 핵심적인 답변을 작성하세요.\\n\\n\"\n",
    "    f\"문맥:\\n{context_raw}\\n\\n\"\n",
    "    f\"질문: {demo_q['q']}\\n\\n\"\n",
    "    \"정답:\"\n",
    ")\n",
    "\n",
    "# OpenAI ChatCompletion으로 답변 생성\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # or gpt-4-turbo, gpt-4o 등 선택 가능\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 금융 분야 전문 AI 어시스턴트입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "ans_raw = response.choices[0].message.content.strip()\n",
    "print(\"[압축 전 답변]\\n\", ans_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "XZkvxwZlWTz2",
   "metadata": {
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1763006262307,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "XZkvxwZlWTz2"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# 혹은 아래와 같은 방식 가능\n",
    "# os.environ[\"OPENAI_API_KEY\"] =userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "def gen(prompt, max_length=512, model=\"gpt-4o-mini\"):\n",
    "    client = OpenAI()\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt,\n",
    "        max_output_tokens=max_length,\n",
    "    )\n",
    "\n",
    "    generated = response.output_text\n",
    "\n",
    "    return [{\"generated_text\": generated}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126dc4d",
   "metadata": {
    "id": "7126dc4d"
   },
   "source": [
    "\n",
    "## 11) LLMLingua 컨텍스트 압축\n",
    "\n",
    "target_token 또는 compression_rate 로 컨텍스트 토큰 수를 줄여 비용/지연 절감\n",
    "keep_keywords 로 도메인 핵심 용어 보존\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rC8z6z1-UWSE",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1763007811383,
     "user": {
      "displayName": "HANWOOL LEE",
      "userId": "01859293415360821440"
     },
     "user_tz": -540
    },
    "id": "rC8z6z1-UWSE"
   },
   "outputs": [],
   "source": [
    "# GPU Out of Memory 발생 시 아래 코드 활용\n",
    "# Python 레퍼런스 삭제\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "for obj in [v for v in globals().values() if hasattr(v, 'to')]:\n",
    "    del obj\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ca678e9",
   "metadata": {
    "id": "6ca678e9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd5190da95a4830bd04189cea90e11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축 전 토큰 추정: 33\n",
      "압축 후 토큰 추정: 37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "compressor = PromptCompressor()\n",
    "compressed = compressor.compress_prompt(\n",
    "    context=[context_raw],   # 반드시 리스트로 감싸기\n",
    "    instruction=f\"다음 문맥은 질문 `{demo_q['q']}` 에 답하기 위한 근거다. 핵심 사실만 남기고 불필요한 서술을 제거하라.\",\n",
    "    question=demo_q['q'],    # 질문은 question 인자로 넣기\n",
    "    target_token=10         # 토큰 목표치\n",
    ")\n",
    "context_cmp = compressed[\"compressed_prompt\"]\n",
    "print(\"압축 전 토큰 추정:\", len(context_raw.split()))\n",
    "print(\"압축 후 토큰 추정:\", len(context_cmp.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ddccbc0",
   "metadata": {
    "id": "1ddccbc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[압축 후 답변]\n",
      " HBM 채택 확대는 LLM 추론 성능과 메모리 대역폭 병목을 개선한다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_cmp = f\"문맥을 바탕으로 질문에 자세하고 핵심적인 답해줘.\\n문맥:\\n{context_cmp}\\n\\n질문:{demo_q['q']}\\n정답:\"\n",
    "ans_cmp = gen(prompt_cmp, max_length=1024)[0]['generated_text']\n",
    "print(\"[압축 후 답변]\\n\", ans_cmp[:600])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e7a50",
   "metadata": {
    "id": "b00e7a50"
   },
   "source": [
    "\n",
    "## 12) RAGAS 평가(라이트)\n",
    "\n",
    "Retrieval: context_precision, context_recall\n",
    "E2E: answer_semantic_similarity (로컬 임베딩 기반)\n",
    "주의: 버전에 따라 임베딩 백엔드 설정이 필요할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ePr0T8OCdY6B",
   "metadata": {
    "id": "ePr0T8OCdY6B",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in /opt/conda/lib/python3.10/site-packages (0.3.9)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from ragas) (1.26.3)\n",
      "Requirement already satisfied: datasets>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ragas) (4.4.1)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from ragas) (0.12.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ragas) (2.12.4)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from ragas) (0.20.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from ragas) (14.2.0)\n",
      "Requirement already satisfied: openai>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ragas) (2.7.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from ragas) (4.67.1)\n",
      "Requirement already satisfied: instructor in /opt/conda/lib/python3.10/site-packages (from ragas) (1.13.0)\n",
      "Requirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (from ragas) (3.1.45)\n",
      "Requirement already satisfied: pillow>=10.4.0 in /opt/conda/lib/python3.10/site-packages (from ragas) (12.0.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from ragas) (3.1)\n",
      "Requirement already satisfied: scikit-network in /opt/conda/lib/python3.10/site-packages (from ragas) (0.33.3)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (from ragas) (0.2.14)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (from ragas) (0.2.43)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.10/site-packages (from ragas) (0.2.12)\n",
      "Requirement already satisfied: langchain_openai in /opt/conda/lib/python3.10/site-packages (from ragas) (1.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0->ragas) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=4.0.0->ragas) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.0.0->ragas) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.0.0->ragas) (1.8.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.0.0->ragas) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.0.0->ragas) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai>=1.0.0->ragas) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->ragas) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->ragas) (0.4.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython->ragas) (4.0.12)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (3.13.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (0.17.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (3.1.6)\n",
      "Requirement already satisfied: pre-commit>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (4.4.0)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (8.5.0)\n",
      "Requirement already satisfied: ty>=0.0.1a23 in /opt/conda/lib/python3.10/site-packages (from instructor->ragas) (0.0.1a26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->ragas) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->ragas) (2.15.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer->ragas) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->ragas) (1.5.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (2.0.44)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.1.147)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading langchain_openai-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.29-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.26-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.23-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.22-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.21-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.20-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.18-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.17-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.16-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.15-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.24-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.23-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.22-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_openai-0.0.2.post1-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_openai-0.0.2-py3-none-any.whl.metadata (570 bytes)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.3.22-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (0.4.3)\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.12-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas)\n",
      "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "  Downloading langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.2.18-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.15-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from ragas)\n",
      "  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_community-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.6-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading langchain_community-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading langchain_community-0.2.2-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n",
      "  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\n",
      "  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\n",
      "  Downloading langchain_community-0.0.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.18-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.17-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain_community-0.0.15-py3-none-any.whl.metadata (7.6 kB)\n",
      "  Downloading langchain_community-0.0.14-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.13-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading langchain_community-0.0.11-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.10-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.8-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "  Downloading langchain_community-0.0.6-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading langchain_community-0.0.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain_community-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_community-0.0.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Downloading langchain_community-0.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Downloading langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain (from ragas)\n",
      "  Using cached langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Using cached langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (1.0.3)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core->ragas)\n",
      "  Using cached langsmith-0.4.42-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (1.0.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (2.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->ragas) (2025.11.3)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/conda/lib/python3.10/site-packages (from scikit-network->ragas) (1.15.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->ragas) (1.22.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0->ragas) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0->ragas) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->ragas) (5.0.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=4.0.0->ragas) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=4.0.0->ragas) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=4.0.0->ragas) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=4.0.0->ragas) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.4->instructor->ragas) (2.1.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (2.1)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community->ragas)\n",
      "  Using cached langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragas) (0.2.9)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->ragas) (0.25.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->ragas) (0.1.2)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit>=4.3.0->instructor->ragas) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit>=4.3.0->instructor->ragas) (2.6.15)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from pre-commit>=4.3.0->instructor->ragas) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit>=4.3.0->instructor->ragas) (20.35.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->ragas) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=4.0.0->ragas) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=4.0.0->ragas) (2.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=4.0.0->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=4.0.0->ragas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=4.0.0->ragas) (2025.2)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->ragas) (1.12.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0.0->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.1.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (0.4.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor->ragas) (3.10.0)\n",
      "Using cached langchain-1.0.5-py3-none-any.whl (93 kB)\n",
      "Using cached langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Using cached langsmith-0.4.42-py3-none-any.whl (401 kB)\n",
      "Using cached langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.147\n",
      "    Uninstalling langsmith-0.1.147:\n",
      "      Successfully uninstalled langsmith-0.1.147\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.43\n",
      "    Uninstalling langchain-core-0.2.43:\n",
      "      Successfully uninstalled langchain-core-0.2.43\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.4\n",
      "    Uninstalling langchain-text-splitters-0.2.4:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.4\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.12\n",
      "    Uninstalling langchain-community-0.2.12:\n",
      "      Successfully uninstalled langchain-community-0.2.12\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.14\n",
      "    Uninstalling langchain-0.2.14:\n",
      "      Successfully uninstalled langchain-0.2.14\n",
      "Successfully installed langchain-1.0.5 langchain-community-0.4.1 langchain-core-1.0.4 langchain-text-splitters-1.0.0 langsmith-0.4.42\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U ragas\n",
    "\n",
    "#아래 코드는 ragas 최신 버전에서 동작, 다만 colab에서는 ragas==0.3.9만 지원하기에 코드는 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87176f",
   "metadata": {
    "id": "cd87176f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_relevance,\n",
    ")\n",
    "from ragas.llms import OpenAI as RagasOpenAI\n",
    "from ragas.embeddings import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "ragas_llm = RagasOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "ragas_embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "def gen(prompt):\n",
    "    res = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    return res.choices[0].message.content\n",
    "\n",
    "samples = []\n",
    "for qobj in queries[:3]:\n",
    "    q_txt = qobj[\"q\"]\n",
    "\n",
    "    hyb = hybrid_scores(embed_model, index_manual, q_txt, lam=0.5)\n",
    "    order = np.argsort(hyb)[::-1][:3]\n",
    "    ctx_list = [docs_texts[i] for i in order]\n",
    "\n",
    "    ctx_join = \"\\n\\n\".join(ctx_list)\n",
    "    prompt = f\"맥락:\\n{ctx_join}\\n\\n질문:{q_txt}\\n답변:\"\n",
    "    ans = gen(prompt)\n",
    "\n",
    "    gt = \" \".join(qobj.get(\"needs\", {}).get(\"tags\", []))\n",
    "\n",
    "    samples.append(\n",
    "        {\n",
    "            \"question\": q_txt,\n",
    "            \"contexts\": ctx_list,\n",
    "            \"answer\": ans,\n",
    "            \"ground_truth\": gt,\n",
    "        }\n",
    "    )\n",
    "\n",
    "ds = Dataset.from_list(samples)\n",
    "\n",
    "result = evaluate(\n",
    "    ds,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        answer_relevance,\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "result.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c589f2",
   "metadata": {
    "id": "39c589f2"
   },
   "source": [
    "\n",
    "## 13) 임베딩 A/B 비교 (NMIXX vs Baseline 폴백)\n",
    "\n",
    "동일 코퍼스/질의/지표(P@k, R@k, MRR, nDCG@k)로 두 임베딩을 비교\n",
    "NMIXX가 없으면 자동으로 BGE vs MiniLM 비교로 폴백\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd8be9b3",
   "metadata": {
    "id": "cd8be9b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nmixx-fin/nmixx-bge-m3. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/B 대상: ['nmixx-fin/nmixx-bge-m3', 'BAAI/bge-base-en-v1.5']\n",
      "[BUILD] nmixx-fin/nmixx-bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name nmixx-fin/nmixx-bge-m3. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BUILD] BAAI/bge-base-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def build_index_for(model_id):\n",
    "    \"특정 모델 ID에 대해 임베딩/인덱스 구성\"\n",
    "    m = SentenceTransformer(model_id)\n",
    "    idx, vec = build_faiss(m, docs_texts)\n",
    "    return m, idx, vec\n",
    "\n",
    "ab_candidates = []\n",
    "try:\n",
    "    _m, _idx, _vec = build_index_for(NMIXX_HUB_ID)\n",
    "    ab_candidates.append(NMIXX_HUB_ID)\n",
    "except Exception as e:\n",
    "    print(\"[WARN] NMIXX 로드 실패:\", e)\n",
    "\n",
    "if \"BAAI/bge-base-en-v1.5\" not in ab_candidates:\n",
    "    ab_candidates.append(\"BAAI/bge-base-en-v1.5\")\n",
    "if len(ab_candidates) < 2:\n",
    "    ab_candidates.append(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"A/B 대상:\", ab_candidates[:2])\n",
    "\n",
    "models_ab = OrderedDict()\n",
    "for mid in ab_candidates[:2]:\n",
    "    print(\"[BUILD]\", mid)\n",
    "    m, idx, vec = build_index_for(mid)\n",
    "    models_ab[mid] = {\"model\": m, \"index\": idx, \"vec\": vec}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d116ab4d",
   "metadata": {
    "id": "d116ab4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mode</th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>MRR</th>\n",
       "      <th>nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nmixx-fin/nmixx-bge-m3</td>\n",
       "      <td>hybrid_0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.858365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nmixx-fin/nmixx-bge-m3</td>\n",
       "      <td>vector</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model        mode   P@5  R@5  MRR    nDCG@5\n",
       "2   BAAI/bge-base-en-v1.5  hybrid_0.5  0.32  1.0  1.0  0.977438\n",
       "0  nmixx-fin/nmixx-bge-m3  hybrid_0.5  0.32  1.0  1.0  0.973325\n",
       "3   BAAI/bge-base-en-v1.5      vector  0.32  1.0  0.8  0.858365\n",
       "1  nmixx-fin/nmixx-bge-m3      vector  0.32  1.0  1.0  1.000000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def eval_ab(models_ab, lam=0.5, k=5, use_rewrite=True):\n",
    "    \"각 모델에 대해 vector/hybrid 성능을 측정하고 평균 비교\"\n",
    "    table = []\n",
    "    for mid, pack in models_ab.items():\n",
    "        m, idx = pack[\"model\"], pack[\"index\"]\n",
    "        rows = []\n",
    "        for qobj in queries:\n",
    "            q = qobj[\"q\"]; needs = qobj[\"needs\"]\n",
    "            subs = rule_rewrite(q) if use_rewrite else [q]\n",
    "            vec = np.zeros(len(docs_texts)); bm = np.zeros(len(docs_texts))\n",
    "            for sub in subs:\n",
    "                vec = np.maximum(vec, cosine_scores(m, idx, sub))\n",
    "                bm  = np.maximum(bm, bm25_scores(sub))\n",
    "            order,_ = topk_by_scores(vec, k=k); rows.append(metrics_at_k(order, needs, k=k) | {\"mode\":\"vector\"})\n",
    "            hyb = lam*((bm-bm.min())/(bm.ptp()+1e-9)) + (1-lam)*((vec-vec.min())/(vec.ptp()+1e-9))\n",
    "            order,_ = topk_by_scores(hyb, k=k); rows.append(metrics_at_k(order, needs, k=k) | {\"mode\":f\"hybrid_{lam}\"})\n",
    "        df = pd.DataFrame(rows).assign(model=mid)\n",
    "        agg = df.groupby([\"model\",\"mode\"]).mean(numeric_only=True).reset_index()\n",
    "        table.append(agg)\n",
    "    return pd.concat(table, ignore_index=True)\n",
    "\n",
    "df_ab = eval_ab(models_ab, lam=0.5, k=5, use_rewrite=True)\n",
    "df_ab.sort_values([\"mode\",\"model\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
